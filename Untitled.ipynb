{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification using Neural Networks\n",
    "\n",
    "\n",
    "##### Agenda:\n",
    "\n",
    "1.Image Classification<br>\n",
    "2.MNIST Dataset<br>\n",
    "3.Linear Classification<br>\n",
    "4.Neural Networks<br>\n",
    "5.Neural Network Architechture<br>\n",
    "6.Neural Network Learning<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<H3>Image Classification</H3>\n",
    "\n",
    "<H4>Motivation</H4> In this section we will introduce the Image Classification problem, which is the task of assigning an input image one label from a fixed set of categories. This is one of the core problems in Computer Vision that, despite its simplicity, has a large variety of practical applications.\n",
    "\n",
    "<H4>Example</H4> For example, in the image below an image classification model takes a single image and assigns probabilities to 4 labels, <b>{cat, dog, hat, mug}</b>.\n",
    "<img src='images/classify.png' >\n",
    "As shown in the image, keep in mind that to a computer an image is represented as one large 3-dimensional array of numbers. In this example, the cat image is 248 pixels wide, 400 pixels tall, and has three color channels Red,Green,Blue (or RGB for short). Therefore, the image consists of <b>248 x 400 x 3 numbers</b>, or a total of 297,600 numbers. Each number is an integer that ranges from 0 (black) to 255 (white). \n",
    "\n",
    "Our task is to turn this quarter of a million numbers into a single label, such as \"cat\".\n",
    "\n",
    "\n",
    "<H4>Challenge.</H4> Since this task of recognizing a visual concept (e.g. cat) is relatively trivial for a human to perform, it is worth considering the challenges involved from the perspective of a Computer Vision algorithm. As we present (an inexhaustive) list of challenges below, keep in mind the raw representation of images as a 3-D array of brightness values:\n",
    "\n",
    "<ul>\n",
    "<li><b>Viewpoint variation.</b>. A single instance of an object can be oriented in many ways with respect to the camera.</li>\n",
    "<li><b>Scale variation.</b> Visual classes often exhibit variation in their size (size in the real world, not only in terms of their extent in the image).</li>\n",
    "<li><b>Deformation.</b>. Many objects of interest are not rigid bodies and can be deformed in extreme ways.</li>\n",
    "<li><b>Occlusion.</b> The objects of interest can be occluded. Sometimes only a small portion of an object (as little as few pixels) could be visible.</li>\n",
    "<li><b>Illumination conditions.</b>. The effects of illumination are drastic on the pixel level.</li>\n",
    "<li><b>Background clutter.</b> The objects of interest may blend into their environment, making them hard to identify.</li>\n",
    "<li><b>Intra-class variation.</b> The classes of interest can often be relatively broad, such as chair. There are many different types of these objects, each with their own appearance.</li>\n",
    "</ul>\n",
    "<img src='images/challenges.jpeg' >\n",
    "<br>\n",
    "<br>\n",
    "A good image classification model must be invariant to the cross product of all these variations, while simultaneously retaining sensitivity to the inter-class variations.\n",
    "\n",
    "\n",
    "<b>Data-driven approach.</b> How might we go about writing an algorithm that can classify images into distinct categories? Unlike writing an algorithm for, for example, sorting a list of numbers, it is not obvious how one might write an algorithm for identifying cats in images. Therefore, instead of trying to specify what every one of the categories of interest look like directly in code, the approach that we will take is not unlike one you would take with a child: we're going to provide the computer with many examples of each class and then develop learning algorithms that look at these examples and learn about the visual appearance of each class. This approach is referred to as a data-driven approach, since it relies on first accumulating a training dataset of labeled images. Here is an example of what such a dataset might look like:\n",
    "<br>\n",
    "<img src='images/trainset.jpg' alt ='An example training set for four visual categories. In practice we may have thousands of categories and hundreds of thousands of images for each category.'>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<H3>MNIST Dataset</H3>\n",
    "\n",
    "MNIST dataset\n",
    "The MNIST database (Mixed National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems.\n",
    "<br>\n",
    "Each image in the databse is of size 28*28 pixels (i.e,784 pixels)\n",
    "<br>\n",
    "The  database contains 60,000 training images and 10,000 testing images.\n",
    "<br>\n",
    "\n",
    "\n",
    "<img src='images/mnist.png'>\n",
    "<img src='images/mnist_performance.jpg'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3> Linear Classification</H3>\n",
    "<H4>Parameterized mapping from images to label scores</H4>\n",
    "The first component of this approach is to define the score function that maps the pixel values of an image to confidence scores for each class.\n",
    "As before, let's assume a training dataset of images $x_i \\in R^D$, each associated with a label $y_i$. Here $i = 1 \\dots N$ and $y_i∈1…K$ $y_i∈1…K$.That is, we have N examples (each with a dimensionality D) and K distinct categories. For example, in CIFAR-10 we have a training set of N = 50,000 images, each with D = 32 x 32 x 3 = 3072 pixels, and K = 10, since there are 10 distinct classes (dog, cat, car, etc). We will now define the score function $f: R^D \\mapsto R^K$ that maps the raw image pixels to class scores.\n",
    "\n",
    "<H4>Linear Classifier</H4>\n",
    "\n",
    "$$f(x_i, W, b) =  W x_i + b$$\n",
    "\n",
    "In the above equation, we are assuming that the image xixi has all of its pixels flattened out to a single column vector of shape [D x 1]. The matrix W (of size [K x D]), and the vector b (of size [K x 1]) are the parameters of the function. In CIFAR-10, xixi contains all pixels in the i-th image flattened into a single [3072 x 1] column, W is [10 x 3072] and b is [10 x 1], so 3072 numbers come into the function (the raw pixel values) and 10 numbers come out (the class scores). The parameters in W are often called the weights, and b is called the bias vector because it influences the output scores, but without interacting with the actual data $x_i$. However, you will often hear people use the terms weights and parameters interchangeably.\n",
    "\n",
    "<H4>Interpreting a linear classifier</H4>\n",
    "Notice that a linear classifier computes the score of a class as a weighted sum of all of its pixel values across all 3 of its color channels. Depending on precisely what values we set for these weights, the function has the capacity to like or dislike (depending on the sign of each weight) certain colors at certain positions in the image. For instance, you can imagine that the \"ship\" class might be more likely if there is a lot of blue on the sides of an image (which could likely correspond to water). You might expect that the \"ship\" classifier would then have a lot of positive weights across its blue channel weights (presence of blue increases score of ship), and negative weights in the red/green channels (presence of red/green decreases the score of ship).\n",
    "\n",
    "<img src='images/imagemap.jpg'>\n",
    "\n",
    "An example of mapping an image to class scores. For the sake of visualization, we assume the image only has 4 pixels (4 monochrome pixels), and that we have 3 classes (red (cat), green (dog), blue (ship) class).We stretch the image pixels into a column and perform matrix multiplication to get the scores for each class. Note that this particular set of weights W is not good at all: the weights assign our cat image a very low cat score. In particular, this set of weights seems convinced that it's looking at a dog\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t\t\n",
    "<H5>Analogy of images as high-dimensional points.</H5> Since the images are stretched into high-dimensional column vectors, we can interpret each image as a single point in this space (e.g. each image in CIFAR-10 is a point in 3072-dimensional space of 32x32x3 pixels). Analogously, the entire dataset is a (labeled) set of points.\n",
    "<img src='images/pixelspace.jpeg'>\n",
    "\n",
    "Since we defined the score of each class as a weighted sum of all image pixels, each class score is a linear function over this space. We cannot visualize 3072-dimensional spaces, but if we imagine squashing all those dimensions into only two dimensions, then we can try to visualize what the classifier might be doing:\n",
    "\n",
    "\n",
    "As we saw above, every row of $W$ is a classifier for one of the classes. The geometric interpretation of these numbers is that as we change one of the rows of $W$, the corresponding line in the pixel space will rotate in different directions. The biases bb, on the other hand, allow our classifiers to translate the lines. In particular, note that without the bias terms, plugging in $x_i=0$ $_xi=0$ would always give score of zero regardless of the weights, so all lines would be forced to cross the origin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Optimization\n",
    "\n",
    "To reiterate, the loss function lets us quantify the quality of any particular set of weights W. \n",
    "The goal of optimization is to find W that minimizes the loss function. \n",
    "We will now motivate and slowly develop an approach to optimizing the loss function. \n",
    "\n",
    "\n",
    "\n",
    "Strategy #1: A first very bad idea solution: Random search\n",
    "Strategy #2: Random Local Search\n",
    "Strategy #3: Following the Gradient i.e, Update in negative gradient direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying a learning algorithm...\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import random\n",
    "from numpy import arange\n",
    "#from classification import *\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "\n",
    "##################################Takes time to run. So have to refactor the code ##################################\n",
    "def run():\n",
    "    mnist = fetch_mldata('MNIST original')\n",
    "    #mnist.data, mnist.target = shuffle(mnist.data, mnist.target)\n",
    "    #print mnist.data.shape\n",
    "    # Trunk the data\n",
    "    n_train = 60000\n",
    "    n_test = 10000\n",
    "\n",
    "    # Define training and testing sets\n",
    "    indices = arange(len(mnist.data))\n",
    "    random.seed(0)\n",
    "    #train_idx = random.sample(indices, n_train)\n",
    "    #test_idx = random.sample(indices, n_test)\n",
    "    train_idx = arange(0,n_train)\n",
    "    test_idx = arange(n_train+1,n_train+n_test)\n",
    "\n",
    "    X_train, y_train = mnist.data[train_idx], mnist.target[train_idx]\n",
    "    X_test, y_test = mnist.data[test_idx], mnist.target[test_idx]\n",
    "\n",
    "    # Apply a learning algorithm\n",
    "    print \"Applying a learning algorithm...\"\n",
    "    clf = svm.SVC(gamma=0.001)\n",
    "    #clf = RandomForestClassifier(n_estimators=10,n_jobs=2)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make a prediction\n",
    "    print \"Making predictions...\"\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    #print y_pred\n",
    "\n",
    "    # Evaluate the prediction\n",
    "    print \"Evaluating results...\"\n",
    "    print \"Precision: \\t\", metrics.precision_score(y_test, y_pred)\n",
    "    print \"Recall: \\t\", metrics.recall_score(y_test, y_pred)\n",
    "    print \"F1 score: \\t\", metrics.f1_score(y_test, y_pred)\n",
    "    print \"Mean accuracy: \\t\", clf.score(X_test, y_test)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    results = run()\n",
    "    end_time = time.time()\n",
    "    print \"Overall running time:\", end_time - start_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<H3>Neural Networks</H3>\n",
    "\n",
    "\n",
    "Basic architechure of an Artificial Neural network\n",
    "<img src='images/neural.png'>\n",
    "An artificial neural network is an interconnected group of nodes, akin to the vast network of neurons in a brain. Here, each circular node represents an artificial neuron and an arrow represents a connection from the output of one neuron to the input of another.\n",
    "\n",
    "\n",
    "<H5>An Artificial neuron</H5>\n",
    "<img src='images/neuron2.png'>\n",
    "$$output=f(input*W+b)$$\n",
    "\n",
    "<b>W</b> is called the wieght <br>\n",
    "<b>b</b> is called the bias <br>\n",
    "<b>f</b> is the Activation function <br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<H3>Neural Network architectures</H3>\n",
    "<H5>Layer-wise organization</H5>\n",
    "Neural Networks as neurons in graphs. Neural Networks are modeled as collections of neurons that are connected in an acyclic graph. In other words, the outputs of some neurons can become inputs to other neurons. Cycles are not allowed since that would imply an infinite loop in the forward pass of a network. Instead of an amorphous blobs of connected neurons, Neural Network models are often organized into distinct layers of neurons. For regular neural networks, the most common layer type is the fully-connected layer in which neurons between two adjacent layers are fully pairwise connected, but neurons within a single layer share no connections. Below are two example Neural Network topologies that use a stack of fully-connected layers:\n",
    "\n",
    "<img src='images/neural_net.jpeg'>\n",
    "\n",
    "<H6>Naming conventions.</H6> Notice that when we say N-layer neural network, we do not count the input layer. Therefore, a single-layer neural network describes a network with no hidden layers (input directly mapped to output). In that sense, you can sometimes hear people say that logistic regression or SVMs are simply a special case of single-layer Neural Networks. You may also hear these networks interchangeably referred to as \"Artificial Neural Networks\" (ANN) or \"Multi-Layer Perceptrons\" (MLP). Many people do not like the analogies between Neural Networks and real brains and prefer to refer to neurons as units.\n",
    "\n",
    "<H6>Output layer.</H6> Unlike all layers in a Neural Network, the output layer neurons most commonly do not have an activation function (or you can think of them as having a linear identity activation function). This is because the last output layer is usually taken to represent the class scores (e.g. in classification), which are arbitrary real-valued numbers, or some kind of real-valued target (e.g. in regression).\n",
    "\n",
    "<H6>Sizing neural networks.</H6> The two metrics that people commonly use to measure the size of neural networks are the number of neurons, or more commonly the number of parameters. Working with the two example networks in the above picture:\n",
    "\n",
    "The first network (left) has 4 + 2 = 6 neurons (not counting the inputs), [3 x 4] + [4 x 2] = 20 weights and 4 + 2 = 6 biases, for a total of 26 learnable parameters.\n",
    "The second network (right) has 4 + 4 + 1 = 9 neurons, [3 x 4] + [4 x 4] + [4 x 1] = 12 + 16 + 4 = 32 weights and 4 + 4 + 1 = 9 biases, for a total of 41 learnable parameters.\n",
    "To give you some context, modern Convolutional Networks contain on orders of 100 million parameters and are usually made up of approximately 10-20 layers (hence deep learning). However, as we will see the number of effective connections is significantly greater due to parameter sharing. More on this in the Convolutional Neural Networks module.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<H5>Activations</H5>\n",
    "Differenct Activation functions used in practice.\n",
    "Step function\n",
    "<img src='images/step.png'>\n",
    "Rectified Linear function\n",
    "<img src='images/relu.png'>\n",
    "Sigmoid Function\n",
    "<img src='images/sigmoid.png'>\n",
    "Hyperbolic Tangent function\n",
    "<img src='images/tanh.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<H4>Feed Fowrward Computation</H4>\n",
    "An example neural network consissting of 2 hidder neurons and a single out put neuron.\n",
    "\n",
    "<img src='images/2neuron1.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xe688390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from ipywidgets import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def f(w1,b1,w2,b2,w3_1,w3_2):\n",
    "     input=np.arange(-1,1,0.01)\n",
    "     y1=input*w1+b1\n",
    "     z1=1.0/(1.0 + np.exp(-y1))\n",
    "     y2=input*w2+b2\n",
    "     z2=1.0/(1.0 + np.exp(-y2))\n",
    "     plt.plot(input,w3_1*z1+w3_2*z2)\n",
    "     plt.show()\n",
    "\n",
    "interact(f, w1=[-100,100],b1=[-50,50],w2=[-100,100],b2=[-50,50],w3_1=[-100,100],w3_2=[-100,100]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<H3>Setting number of layers and their sizes</H3>\n",
    "\n",
    "How do we decide on what architecture to use when faced with a practical problem? Should we use no hidden layers? One hidden layer? Two hidden layers? How large should each layer be? First, note that as we increase the size and number of layers in a Neural Network, the capacity of the network increases. That is, the space of representable functions grows since the neurons can collaborate to express many different functions. For example, suppose we had a binary classification problem in two dimensions. We could train three separate neural networks, each with one hidden layer of some size and obtain the following classifiers:\n",
    "\n",
    "<img src='images/layer_sizes.jpeg'>\n",
    "\n",
    "Larger Neural Networks can represent more complicated functions. The data are shown as circles colored by their class, and the decision regions by a trained neural network are shown underneath. You can play with these examples in this \n",
    "<a href=http://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html>ConvNetsJS demo. </a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "'''Trains a simple deep NN on the MNIST dataset.\n",
    "Gets to 98.40% test accuracy after 20 epochs\n",
    "(there is *a lot* of margin for parameter tuning).\n",
    "2 seconds per epoch on a K520 GPU.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "nb_classes = 10\n",
    "nb_epoch = 20\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(784,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    verbose=1, validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
